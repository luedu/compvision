{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple neural network for cats vs dogs classification\n",
    "https://www.pyimagesearch.com/2016/09/26/a-simple-neural-network-with-python-and-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_feature_vector(image, size=(32, 32)):\n",
    "    # Resize the image to a fixed size, then flatten the image into a list of raw pixel intensities\n",
    "    return cv2.resize(image, size).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'kaggle_dogs_vs_cats/train/'\n",
    "test_path = 'test_images/'\n",
    "output_model_path = 'output/simple_neural_network.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000/25000\n",
      "Processed 2000/25000\n",
      "Processed 3000/25000\n",
      "Processed 4000/25000\n",
      "Processed 5000/25000\n",
      "Processed 6000/25000\n",
      "Processed 7000/25000\n",
      "Processed 8000/25000\n",
      "Processed 9000/25000\n",
      "Processed 10000/25000\n",
      "Processed 11000/25000\n",
      "Processed 12000/25000\n",
      "Processed 13000/25000\n",
      "Processed 14000/25000\n",
      "Processed 15000/25000\n",
      "Processed 16000/25000\n",
      "Processed 17000/25000\n",
      "Processed 18000/25000\n",
      "Processed 19000/25000\n",
      "Processed 20000/25000\n",
      "Processed 21000/25000\n",
      "Processed 22000/25000\n",
      "Processed 23000/25000\n",
      "Processed 24000/25000\n"
     ]
    }
   ],
   "source": [
    "# Grab the list of images that we'll be describing\n",
    "imagePaths = list(paths.list_images(dataset_path))\n",
    "\n",
    "# initialize the data matrix and labels list\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Loop over the input images\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    # Load the image and extract the class label (assuming that our\n",
    "    # path has the format: /path/to/dataset/{class}.{image_num}.jpg\n",
    "    image = cv2.imread(imagePath)\n",
    "    label = imagePath.split(os.path.sep)[-1].split(\".\")[0]\n",
    "    \n",
    "    # Construct a feature vector raw pixel intensities, then update the data matrix and labels list\n",
    "    features = image_to_feature_vector(image)\n",
    "    data.append(features)\n",
    "    labels.append(label)\n",
    "    \n",
    "    # Show an update every 1,000 images\n",
    "    if i > 0 and i % 1000 == 0:\n",
    "        print(\"Processed {}/{}\".format(i, len(imagePaths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels, converting them from strings to integers\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "\n",
    "# Scale the input image pixels to the range [0, 1], then transform the labels into vectors\n",
    "# in the range [0, num_classes] -- this generates a vector for each label where the index\n",
    "# of the label is set to `1` and all other entries to `0`\n",
    "data = np.array(data) / 255.0\n",
    "labels = np_utils.to_categorical(labels, 2)\n",
    "\n",
    "# Partition the data into training and testing splits, using 75%\n",
    "# of the data for training and the remaining 25% for testing\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(data, labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(768, input_dim=3072, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
    "model.add(Dense(384, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Training using Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18750/18750 [==============================] - 1s 78us/step - loss: 0.6838 - accuracy: 0.5627\n",
      "Epoch 2/50\n",
      "18750/18750 [==============================] - 1s 73us/step - loss: 0.6637 - accuracy: 0.5946\n",
      "Epoch 3/50\n",
      "18750/18750 [==============================] - 1s 76us/step - loss: 0.6531 - accuracy: 0.6125\n",
      "Epoch 4/50\n",
      "18750/18750 [==============================] - 1s 76us/step - loss: 0.6465 - accuracy: 0.6229\n",
      "Epoch 5/50\n",
      "18750/18750 [==============================] - 1s 76us/step - loss: 0.6413 - accuracy: 0.6299\n",
      "Epoch 6/50\n",
      "18750/18750 [==============================] - 1s 77us/step - loss: 0.6350 - accuracy: 0.6394\n",
      "Epoch 7/50\n",
      "18750/18750 [==============================] - 1s 77us/step - loss: 0.6313 - accuracy: 0.6422\n",
      "Epoch 8/50\n",
      "18750/18750 [==============================] - ETA: 0s - loss: 0.6268 - accuracy: 0.64 - 1s 79us/step - loss: 0.6267 - accuracy: 0.6457\n",
      "Epoch 9/50\n",
      "18750/18750 [==============================] - 1s 78us/step - loss: 0.6206 - accuracy: 0.6520\n",
      "Epoch 10/50\n",
      "18750/18750 [==============================] - 1s 79us/step - loss: 0.6170 - accuracy: 0.6566\n",
      "Epoch 11/50\n",
      "18750/18750 [==============================] - 2s 80us/step - loss: 0.6142 - accuracy: 0.6642\n",
      "Epoch 12/50\n",
      "18750/18750 [==============================] - 2s 82us/step - loss: 0.6112 - accuracy: 0.6629\n",
      "Epoch 13/50\n",
      "18750/18750 [==============================] - 2s 81us/step - loss: 0.6066 - accuracy: 0.6684\n",
      "Epoch 14/50\n",
      "18750/18750 [==============================] - 2s 82us/step - loss: 0.6054 - accuracy: 0.6672\n",
      "Epoch 15/50\n",
      "18750/18750 [==============================] - 2s 82us/step - loss: 0.5978 - accuracy: 0.6768\n",
      "Epoch 16/50\n",
      "18750/18750 [==============================] - 2s 82us/step - loss: 0.5964 - accuracy: 0.6782\n",
      "Epoch 17/50\n",
      "18750/18750 [==============================] - 2s 83us/step - loss: 0.5943 - accuracy: 0.6775\n",
      "Epoch 18/50\n",
      "18750/18750 [==============================] - 2s 84us/step - loss: 0.5890 - accuracy: 0.6813\n",
      "Epoch 19/50\n",
      "18750/18750 [==============================] - 2s 83us/step - loss: 0.5860 - accuracy: 0.6873\n",
      "Epoch 20/50\n",
      "18750/18750 [==============================] - 2s 85us/step - loss: 0.5803 - accuracy: 0.6932\n",
      "Epoch 21/50\n",
      "18750/18750 [==============================] - 2s 85us/step - loss: 0.5815 - accuracy: 0.6918\n",
      "Epoch 22/50\n",
      "18750/18750 [==============================] - 2s 87us/step - loss: 0.5782 - accuracy: 0.6919\n",
      "Epoch 23/50\n",
      "18750/18750 [==============================] - 2s 85us/step - loss: 0.5752 - accuracy: 0.6973\n",
      "Epoch 24/50\n",
      "18750/18750 [==============================] - 2s 84us/step - loss: 0.5693 - accuracy: 0.7007\n",
      "Epoch 25/50\n",
      "18750/18750 [==============================] - 2s 84us/step - loss: 0.5672 - accuracy: 0.7065\n",
      "Epoch 26/50\n",
      "18750/18750 [==============================] - 2s 83us/step - loss: 0.5635 - accuracy: 0.7066\n",
      "Epoch 27/50\n",
      "18750/18750 [==============================] - 2s 81us/step - loss: 0.5588 - accuracy: 0.7102\n",
      "Epoch 28/50\n",
      "18750/18750 [==============================] - 2s 81us/step - loss: 0.5565 - accuracy: 0.7151\n",
      "Epoch 29/50\n",
      "18750/18750 [==============================] - 2s 82us/step - loss: 0.5591 - accuracy: 0.7120\n",
      "Epoch 30/50\n",
      "18750/18750 [==============================] - 2s 82us/step - loss: 0.5480 - accuracy: 0.7223\n",
      "Epoch 31/50\n",
      "18750/18750 [==============================] - 2s 80us/step - loss: 0.5523 - accuracy: 0.7170\n",
      "Epoch 32/50\n",
      "18750/18750 [==============================] - 2s 82us/step - loss: 0.5448 - accuracy: 0.7229\n",
      "Epoch 33/50\n",
      "18750/18750 [==============================] - 2s 80us/step - loss: 0.5433 - accuracy: 0.7252\n",
      "Epoch 34/50\n",
      "18750/18750 [==============================] - 2s 82us/step - loss: 0.5374 - accuracy: 0.7283\n",
      "Epoch 35/50\n",
      "18750/18750 [==============================] - 1s 80us/step - loss: 0.5336 - accuracy: 0.7340\n",
      "Epoch 36/50\n",
      "18750/18750 [==============================] - 1s 80us/step - loss: 0.5316 - accuracy: 0.7330\n",
      "Epoch 37/50\n",
      "18750/18750 [==============================] - 2s 80us/step - loss: 0.5287 - accuracy: 0.7387\n",
      "Epoch 38/50\n",
      "18750/18750 [==============================] - 2s 81us/step - loss: 0.5256 - accuracy: 0.7394\n",
      "Epoch 39/50\n",
      "18750/18750 [==============================] - 1s 80us/step - loss: 0.5195 - accuracy: 0.7420\n",
      "Epoch 40/50\n",
      "18750/18750 [==============================] - 2s 81us/step - loss: 0.5181 - accuracy: 0.7422\n",
      "Epoch 41/50\n",
      "18750/18750 [==============================] - 2s 83us/step - loss: 0.5160 - accuracy: 0.7441\n",
      "Epoch 42/50\n",
      "18750/18750 [==============================] - 2s 82us/step - loss: 0.5146 - accuracy: 0.7423\n",
      "Epoch 43/50\n",
      "18750/18750 [==============================] - 2s 84us/step - loss: 0.5048 - accuracy: 0.7549\n",
      "Epoch 44/50\n",
      "18750/18750 [==============================] - 2s 89us/step - loss: 0.5044 - accuracy: 0.7562\n",
      "Epoch 45/50\n",
      "18750/18750 [==============================] - 2s 88us/step - loss: 0.5018 - accuracy: 0.7577\n",
      "Epoch 46/50\n",
      "18750/18750 [==============================] - 2s 94us/step - loss: 0.4962 - accuracy: 0.7611\n",
      "Epoch 47/50\n",
      "18750/18750 [==============================] - 2s 90us/step - loss: 0.4960 - accuracy: 0.7601\n",
      "Epoch 48/50\n",
      "18750/18750 [==============================] - 2s 92us/step - loss: 0.4887 - accuracy: 0.7663\n",
      "Epoch 49/50\n",
      "18750/18750 [==============================] - 2s 83us/step - loss: 0.4902 - accuracy: 0.7629\n",
      "Epoch 50/50\n",
      "18750/18750 [==============================] - 2s 92us/step - loss: 0.4820 - accuracy: 0.7679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fcb4246b5f8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGD(lr=0.01)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])  # Binary as there are only two classes\n",
    "model.fit(trainData, trainLabels, epochs=50, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==============================] - 0s 35us/step\n",
      "loss=0.5968, accuracy: 68.1120%\n"
     ]
    }
   ],
   "source": [
    "# Show the accuracy on the testing set\n",
    "(loss, accuracy) = model.evaluate(testData, testLabels, batch_size=128, verbose=1)\n",
    "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "# Dump the network architecture and weights to file\n",
    "model.save(output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the class labels for the Kaggle dogs vs cats dataset\n",
    "CLASSES = [\"cat\", \"dog\"]\n",
    "\n",
    "# Loop over our testing images\n",
    "for imagePath in paths.list_images(test_path):\n",
    "    # Load the image, resize it to a fixed 32 x 32 pixels (ignoring aspect ratio),\n",
    "    # and then extract features from it\n",
    "    image = cv2.imread(imagePath)\n",
    "    features = image_to_feature_vector(image) / 255.0\n",
    "    features = np.array([features])\n",
    "    \n",
    "    # Classify the image using our extracted features and pre-trained neural network\n",
    "    probs = model.predict(features)[0]\n",
    "    prediction = probs.argmax(axis=0)\n",
    "    \n",
    "    # Draw the class and probability on the test image and display it on our screen\n",
    "    label = \"{}: {:.2f}%\".format(CLASSES[prediction], probs[prediction] * 100)\n",
    "    cv2.putText(image, label, (10, 35), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 3)\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
